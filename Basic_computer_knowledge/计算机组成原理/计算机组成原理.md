---
title: 计算机组成原理
date: 2020-12-15 9:59:34
author: DSY
---
<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [计算机组成原理](#计算机组成原理)
  - [一. 总论](#一-总论)
    - [1.1 概念](#11-概念)
    - [1.2 知识地图](#12-知识地图)
      - [1.2.1 计算机的基本组成](#121-计算机的基本组成)
      - [1.2.2 计算机的指令和计算](#122-计算机的指令和计算)
      - [1.2.3 处理器设计](#123-处理器设计)
      - [1.2.4 存储器和 I/O 设备](#124-存储器和-io-设备)
  - [二.计算机的基本组成](#二计算机的基本组成)
    - [2.1 基本硬件组成](#21-基本硬件组成)
      - [2.1.1 主要](#211-主要)
      - [2.1.2 次要](#212-次要)
    - [2.2 冯·诺依曼体系](#22-冯诺依曼体系)
    - [2.3 性能](#23-性能)
      - [2.3.1 指标](#231-指标)
      - [2.3.2 计算机的计时单位: `CPU时钟`](#232-计算机的计时单位-cpu时钟)
      - [2.3.3 性能提升思路](#233-性能提升思路)
      - [2.3.4 功耗](#234-功耗)
      - [2.3.5 提升性能的方法](#235-提升性能的方法)
  - [三. 计算机指令和运算](#三-计算机指令和运算)
    - [3.1 计算机指令 (Instruction Code)](#31-计算机指令-instruction-code)
      - [3.1.1 用来做什么](#311-用来做什么)
      - [3.1.2 计算机指令集 (Instruction Set)](#312-计算机指令集-instruction-set)
      - [3.1.3 存储程序型计算机 (Stored-program Computer)](#313-存储程序型计算机-stored-program-computer)
      - [3.1.4 程序如何变成计算机指令](#314-程序如何变成计算机指令)
      - [3.1.5 常见指令分类](#315-常见指令分类)
      - [3.1.6 汇编器把对应的汇编代码翻译成为机器码](#316-汇编器把对应的汇编代码翻译成为机器码)

<!-- /code_chunk_output -->

# 计算机组成原理

## 一. 总论

### 1.1 概念

> 计算机组成原理的英文叫 Computer Organization。这里的 Organization 是"组织机构"的意思。
> 计算机是由很多个不同部件放在一起，变成了一个"组织机构"。这个组织机构最终能够进行各种计算、控制、读取输入，进行输出，达成各种强大的功能。
>组成原理是计算机体系结构的入门课程。`冯·诺依曼体系`结构确立了现代计算机`硬件`的`基础架构`。因此，学习计算机组成原理，其实就是学习和拆解冯·诺依曼体系结构。

### 1.2 知识地图

![计算机组成原理知识地图](image/计算机组成原理知识地图.jpg)
从这张图可以看出，整个计算机组成原理，就是==围绕着计算机是如何组织运作展开的==，整个计算机组成原理的知识点被拆分成了`四大部分`:

#### 1.2.1 计算机的基本组成

需要学习计算机是由哪些硬件组成的。这些硬件，又是怎么对应到经典的冯·诺依曼体系结构中的，也就是`运算器`、`控制器`、`存储器`、`输入设备`和`输出设备`这五大基本组件。除此之外，还需要了解计算机的两个==核心指标==，`性能`和`功耗`。性能和功耗也是应用和设计五大基本组件中需要 **重点** 考虑的因素。

#### 1.2.2 计算机的指令和计算

**指令**
: 需要搞明白，一行行 C、Java、PHP 程序，是怎么在计算机里面跑起来的。这里面，既需要==了解程序是怎么通过编译器和汇编器，变成一条条机器指令这样的编译过程==，还需要知道==操作系统是怎么链接、装载、执行这些程序的==。而这一条条指令执行的控制过程，就是由计算机五大组件之一的`控制器`来控制的。

**计算**
: 从二进制和编码开始，理解数据在计算机里的表示，以及是怎么从数字电路层面，实现加法、乘法这些基本的运算功能的。实现这些运算功能的算术逻辑单元(Arithmetic Logic Unit/ALU)，也就是计算机五大组件之一的`运算器`。

#### 1.2.3 处理器设计

**CPU 时钟** (CPU Clock)
: 可以用来构造寄存器和内存的锁存器和触发器，因此，CPU 时钟是学习 CPU 的前导知识。需要搞明白为什么需要 CPU 时钟，以及寄存器和内存是用什么样的硬件组成的。

**数据通路**
: 连接了整个`运算器`和`控制器`，并最终组成了 `CPU`。而出于对性能和功耗的考虑，需要进一步理解和掌握面向流水线设计的 CPU、数据和控制冒险，以及分支预测的相关技术。

**CPU**
: ==作为控制器要和输入输出设备通信==，那么就要知道异常和中断发生的机制。所以，在 CPU 设计部分的最后，会**学习指令的并行执行**，看看如何直接在 CPU 层面，通过 **SIMD** 来支持并行计算。

#### 1.2.4 存储器和 I/O 设备

- 通过存储器的层次结构作为基础的框架引导，需要掌握从上到下的 CPU 高速缓存、内存、SSD 硬盘和机械硬盘的工作原理，它们之间的性能差异，以及实际应用中利用这些设备会遇到的挑战。

- 存储器其实很多时候又扮演了输入输出设备的角色，所以需要进一步了解，CPU 和这些存储器之间是如何进行通信的，以及最重要的**性能问题**是怎么一回事。

- 对于存储器，不仅需要它们能够正常工作，还要确保里面的数据不能丢失。于是要掌握如何通过 RAID、Erasure Code、ECC 以及分布式 HDFS，这些不同的技术，来**确保数据的完整性和访问性能**。

- 理解什么是 `IO_WAIT`，如何通过 DMA 来提升程序性能。

## 二.计算机的基本组成

### 2.1 基本硬件组成

#### 2.1.1 主要

**CPU**
: 它是计算机**最重要的核心配件**，计算机的所有"计算"都是由 CPU 来进行的，全名是中央处理器(Central Processing Unit)。

**内存** (Memory)
: 撰写的程序、打开的浏览器、运行的游戏，都要加载到内存里才能运行。程序读取的数据、计算得到的结果，也都要放在内存里。内存越大，能加载的东西自然也就越多。存放在内存里的程序和数据，需要被 CPU 读取，CPU 计算完之后，还要把数据写回到内存。

**主板**
: 主板是一个有着各种各样，有时候多达数十乃至上百个插槽的配件。主要有以下作用:

      1. CPU 要插在主板上，内存也要插在主板上。
      2. 主板的芯片组(Chipset)和总线(Bus)解决了 CPU 和 内存 之间如何通信的问题。芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。
      3. 总线则是实际数据传输的高速公路。因此，总线速度(Bus Speed)决定了数据能传输得多快。

#### 2.1.2 次要

**I/O设备**
: 输入(Input)/ 输出(Output)设备，如: 显示器、键盘、鼠标等。

**硬盘**
: 长久保存数据。

**显卡(Graphics Card)**
: 使用图形界面操作系统的计算机，无论是 Windows、Mac OS 还是 Linux，显卡都是必不可少的。显卡之所以特殊，是因为显卡里有除了 CPU 之外的另一个"处理器"，也就是 `GPU`(Graphics Processing Unit，图形处理器)，GPU 一样可以做各种"计算"的工作。

**南桥(SouthBridge)**
: 主板上的南桥芯片组，来用来==控制 外部 I/O 设备 和 CPU 之间的通信==的。"南桥"芯片的名字很直观，一方面，它在主板上的位置，通常在主板的"南面"。另一方面，它的作用就是作为"桥"，来连接鼠标、键盘以及硬盘这些外部设备和 CPU 之间的通信。以前的主板上通常也有"北桥"芯片，用来作为"桥"，连接 CPU 和内存、显卡之间的通信。不过，随着时间的变迁，现在的主板上的"北桥"芯片的工作，已经被移到了 CPU 的内部，所以现在主板上，已经看不到北桥芯片了。

### 2.2 冯·诺依曼体系

概念
: 无论是电脑、手机、还是服务器，都遵循着同一个"计算机"的抽象概念。也就是，计算机祖师爷之一冯·诺依曼(John von Neumann)提出的 **冯·诺依曼体系结构** (Von Neumann architecture)，也叫`存储程序型计算机`。

历史
: 冯·诺依曼在1945年6月30日，基于当时在秘密开发的 EDVAC 写了一篇报告 **《First Draft of a Report on the EDVAC》** ，描述了他心目中的一台计算机应该长什么样。这篇报告在历史上有个很特殊的简称，叫 **First Draft**。
First Draft 里面说了一台计算机应该有哪些部分组成:

      1. 处理器单元: 一个包含算术逻辑单元(Arithmetic Logic Unit，ALU)和处理器寄存器(Processor Register)的处理器单元(Processing Unit)，用来完成各种算术和逻辑运算。因为它能够完成各种数据的处理或者计算工作，因此也有人把这个叫作数据通路(Datapath)或者运算器。

      2. 控制器单元: 一个包含指令寄存器(Instruction Register)和程序计数器(Program Counter)的控制器单元(Control Unit/CU)，用来控制程序的流程，通常就是不同条件下的分支和跳转。在现代计算机里，上面的算术逻辑单元和这里的控制器单元，共同组成了 CPU。

      3. 存储器: 用来存储数据(Data)和指令(Instruction)的内存。以及更大容量的外部存储，在过去，可能是磁带、磁鼓这样的设备，现在通常就是硬盘。
      
      4. I/O设备: 各种输入和输出设备，以及对应的输入和输出机制。现在无论是使用什么样的计算机，其实都是和输入输出设备在打交道。个人电脑的鼠标键盘是输入设备，显示器是输出设备。我们用的智能手机，触摸屏既是输入设备，又是输出设备。而跑在各种云上的服务器，则是通过网络来进行输入和输出。这个时候，网卡既是输入设备又是输出设备。

> **总结**: 任何一台计算机的任何一个部件都可以归到 `运算器`、 `控制器`、 `存储器`、 `输入设备` 和 `输出设备` 中，所有的现代计算机也都是基于这个基础架构来设计开发的。
>
>**扩展**: 所有的计算机程序，也都可以抽象为从输入设备读取输入信息，通过运算器和控制器来执行存储在存储器里的程序，最终把结果输出到输出设备中。而我们所有撰写的无论高级还是低级语言的程序，也都是基于这样一个抽象框架来进行运作的。

### 2.3 性能

一般把性能，定义成响应时间的倒数，也就是：`性能 = 1 / 响应时间`

#### 2.3.1 指标

计算机的性能，主要用两个 **标准指标** 来衡量:

**响应时间** (Response time) 或 执行时间 (Execution time)
: 执行一个程序，到底需要花多少时间。想要提升这个指标，可以理解为让计算机"跑得更快"。

**吞吐率** (Throughput) 或 带宽 (Bandwidth)
: 在一定的时间范围内，到底能处理多少事情。这里的"事情"，在计算机里就是处理的数据或者执行的程序指令。想要提升这个指标，可以理解为让计算机"搬得更多"。

> 提升吞吐率的办法有很多，大部分时候，只要多加一些机器，多堆一些硬件就好了。但是**响应时间的提升却没有那么容易**。

#### 2.3.2 计算机的计时单位: `CPU时钟`

虽然时间是一个很自然的用来 **衡量性能** 的指标，但是用时间来衡量时，有两个问题:

> **1. 时间不准** :
>> 统计时间是用类似于"掐秒表"一样，记录程序运行结束的时间减去程序开始运行的时间。
>> 计算机可能同时运行着好多个程序，CPU 实际上不停地在各个程序之间进行切换。在这些走掉的时间里面，很可能 CPU 切换去运行别的程序了。
>> 有些程序在运行的时候，可能要从网络、硬盘去读取数据，要等网络和硬盘把数据读出来，给到内存和 CPU。
>
> **解决思路** : 要想准确统计某个程序运行时间，进而去比较两个程序的实际性能，得把这些时间给刨除掉。
>
> **2. 会受到主板、内存这些其他相关硬件的影响** :
>
> **解决思路** : 因为会受到相关硬件的影响，所以需要对"时间"这个可以感知的指标进行 **拆解** ，把程序的 `CPU 执行时间` 变成 `CPU 时钟周期数` (CPU Cycles) 和 `时钟周期时间` (Clock Cycle) 的 `乘积`:
> **程序的 CPU 执行时间 = CPU 时钟周期数 × 时钟周期时间**

**时钟周期时间**
: 在 CPU 内部，有一个叫 **晶体振荡器** (Oscillator Crystal)的东西，简称为晶振。计算机把晶振当成 CPU 内部的电子表来使用。晶振带来的每一次"滴答"，就是时钟周期时间。CPU是按照这个"时钟"提示的时间来进行自己的操作。 **时钟周期越短，CPU也就越快**。

#### 2.3.3 性能提升思路

**思路**:
最简单的提升性能方案，自然是 **缩短时钟周期时间** ，也就是提升主频。不过，这个是软件控制不了的事情，但是，如果能够减少程序需要的 **CPU 时钟周期数量** ，一样能够提升程序性能。

对于 CPU 时钟周期数，可以再做一个分解，把它变成 **"指令数 × 每条指令的平均时钟周期数(Cycles Per Instruction，简称 CPI)"** 。不同的指令需要的 Cycles 是不同的，加法和乘法都对应着一条 CPU 指令，但是乘法需要的 Cycles 就比加法要多，自然也就慢。在这样拆分了之后，程序的 CPU 执行时间就可以变成这样三个部分的乘积:
> **程序的 CPU 执行时间 = 指令数 × CPI × 时钟周期时间**

**解决性能问题**，就是要优化这三者:

- **时钟周期时间** (计算机主频): 这个取决于计算机硬件。所熟知的摩尔定律就一直在不停地提高我们计算机的主频。

- **每条指令的平均周期数`CPI`**: 就是一条指令需要多少CPU Cycles。现代的 CPU 通过 **流水线技术** (Pipeline)，让一条指令需要的 CPU Cycle 尽可能地少。因此，对于 CPI 的优化，也是计算机组成和体系结构中的重要一环。

- **指令数** : 代表执行程序到底需要多少条指令、用哪些指令。这个很多时候就把挑战交给了编译器。同样的代码，编译成计算机指令时候，就有各种不同的表示方式。

#### 2.3.4 功耗

**功耗的增加** :

CPU，一般都被叫作超大规模集成电路（Very-Large-Scale Integration，VLSI）。这些电路，实际上都是一个个晶体管组合而成的。CPU 在计算时，其实就是让晶体管里面的"开关"不断地去"打开"和"关闭"，来组合完成各种运算和功能。可从以下几个方面提升性能:

- **增加密度** : 在 CPU 里，同样的面积里面多放晶体管。
- **提升制程** : 同样的面积下，想要多放一点晶体管，就要把晶体管造得小一点。
- **提升主频** : 也就是让晶体管"打开"和"关闭"得更快一点。

但这三者，都会 **增加功耗** ，带来耗电和散热的问题。因此，在 CPU 里面，能够放下的 **晶体管数量** 和 **晶体管的"开关"频率** 也都是有限的。一个 CPU 的功率，可以用这样一个公式来表示:

> **功耗 ~= 1/2 × 负载电容 × 电压的平方 × 开关频率 × 晶体管数量**

**降低功耗方法** :

> **降低电压** : 在整个功耗的公式里面，功耗和电压的平方是成正比的。这意味着电压下降到原来的 `1/5`，整个的功耗会变成原来的 `1/25` ，**非常关键**。

#### 2.3.5 提升性能的方法

- **并行优化，理解 [阿姆达尔定律](#Amdahl)**

> ==通过并行提高性能是最常见的提升性能的方式==。但是，并不是所有问题，都可以通过并行提高性能来解决。如果想要使用这种思想，需要满足这样几个条件:
>> 1.需要进行的计算，本身可以分解成几个可以并行的任务。
>> 2.需要能够分解好问题，并确保结果能够汇总到一起。
>> 3.在"汇总"这个阶段，是没有办法并行进行的，还是得顺序执行，一步一步来。

- **不受影响的执行时间** : 指的是汇总相加的时间，这部分时间是不能通过并行来优化的。

- **加速大概率事件**

- **通过流水线提高性能** : 把 CPU 指令执行的过程进行拆分，细化运行，也是现代 CPU 在主频没有办法提升那么多的情况下，性能仍然可以得到提升的重要原因之一。

- **通过预测提高性能** : 通过预先猜测下一步该干什么，而不是等上一步运行的结果，提前进行运算，也是让程序跑得更快一点的办法。典型的例子就是在一个循环访问数组的时候，凭经验，也会猜到下一步会访问数组的下一项。"分支和冒险"、"局部性原理"这些 CPU 和存储系统设计方法，其实都是在利用对于未来的"预测"，提前进行相应的操作，来提升程序性能。

## 三. 计算机指令和运算

### 3.1 计算机指令 (Instruction Code)

#### 3.1.1 用来做什么

从软件工程师的角度来讲，CPU 就是一个**执行各种计算机指令的逻辑机器**。这里的计算机指令，就好比一门 CPU 能够听得懂的语言，也可以把它叫作 **机器语言** （Machine Language）。

#### 3.1.2 计算机指令集 (Instruction Set)

**计算机指令集** 是 CPU 支持的语言，不同的 CPU 能够听懂的语言不太一样。比如，个人电脑用的是 Intel 的 CPU，苹果手机用的是 ARM 的 CPU。这两者能听懂的语言就不太一样。

#### 3.1.3 存储程序型计算机 (Stored-program Computer)

一个计算机程序，不可能只有一条指令，而是由成千上万条指令组成的。但是 CPU 里不能一直放着所有指令，所以计算机程序平时是存储在存储器中的。这种程序指令存储在存储器里面的计算机，就叫作 **存储程序型计算机**。

#### 3.1.4 程序如何变成计算机指令

要让一段程序在操作系统上跑起来，需要把整个程序翻译成一个 **汇编语言** （ASM，Assembly Language）程序，这个过程一般叫 **编译（Compile）成汇编代码**。
针对汇编代码，可以再用汇编器（Assembler）翻译成机器码（Machine Code）。这些机器码由"0"和"1"组成的机器语言表示。这一条条机器码，就是一条条的计算机指令。

**不直接编译成机器码的好处** :

- 方便优化和调试: 因为编译器也是工具，也是机器，毕竟是机器生成的程序，不是非常完美的，而汇编是机器指令的助记符，一个汇编指令就对应一条机器指令（特殊指令除外），调试起来肯定会比机器指令方便，这样优化起来也方便。

- 从人脑可分析的粒度来减弱复杂性: 高级语言只需要编译成汇编代码就可以了，汇编代码到机器码的转换是由硬件实现的，实现这样的分层，可以有效地减弱编译器编写的复杂性，提高了效率。

#### 3.1.5 常见指令分类

**常见的指令可以分成以下五类** :

1. **算术类指令** : 加减乘除，在CPU层面，都会变成一条条算术指令。
2. **数据传输类指令** : 给变量赋值、在内存里读写数据，用的都是数据传输类指令。
3. **逻辑类指令** :逻辑上的与或非，都是这一类指令。
4. **条件分支类指令** : if/else、switch，都是套件分支类指令。
5. **无条件跳转指令** : 调用函数时，其实就是发起了一个无条件跳转指令。

![常见指令分类和注释](image/常见指令分类和注释.jpeg)

#### 3.1.6 汇编器把对应的汇编代码翻译成为机器码

使用最简单的 `MIPS` 指令集，来演示机器码是如何生成的。
![MIPS指令集示意图](image/MIPS指令集示意图.jpeg)

MISP的指令是一个 `32位` 的整数，高6位叫 **操作码** (Opcode)，也就是代表这条指令具体是一条什么样的指令，剩下的26位有三种格式，分别是R、I和J。

1. **R指令** : 一般用来做 **算术和逻辑操作** ，里面有读取和写入数据的寄存器的地址。如果是逻辑位移操作，后面还有位移操作的位移量，而最后的功能码，则是在前面的操作码不够的时候，扩展操作码表示对应的具体指令的。
2. **I指令** : 通常是用在 **数据传输、条件分支** ，以及在运算的时候使用的并非变量还是常数的时候。这个时候，没有了位移量和操作码，也没有了第三个寄存器，而是把这三部分直接合并成了一个地址值或者一个常数。
3. **J指令** : **跳转指令** ，高 6 位之外的 26 位都是一个跳转后的地址。

> **例** :
> 以一个最简单的加法算术指令 `add t0,s1,$s2` 为例，下面都用十进制来表示对应的代码。
> 这个指令对应 MIPS 指令里 `opcode` 是 `0`，`rs` 代表第一个寄存器的地址是 `17`，`rt` 代表第二个寄存器的地址是 `18`， `rd` 代表目标的临时寄存器 `t0` 的地址是 **8**。因为不是位移操作，所以位移量是 **0**。把这些数字拼起来，就变成了一个 *MIPS* 的加法指令。
> 为了读起来方便，一般把对应的二进制数，用16进制表示出来。这个例子是 **0X02324020**，这个数字也就是这条指令对应的机器码。
> ![加法算术指令示例](image/加法算术指令示例.jpeg)

**总结** :
> 除了 C 这样的编译型的语言之外，不管是 Python 这样的解释型语言，还是 Java 这样使用虚拟机的语言，其实最终都是由不同形式的程序，把代码，转换成 CPU 能够理解的机器码来执行的。
> 只是解释型语言，是通过解释器在程序运行的时候逐句翻译，而 Java 这样使用虚拟机的语言，则是由虚拟机对编译出来的中间代码进行解释，或者即时编译成为机器码来最终执行。

--------------------------
<span id="Amdahl">1. 阿姆达尔定律
: 是在性能优化中，经常用到的经验定律，对于一个程序进行优化之后，处理器并行运算效率提升之后的情况，具体可以用这样一个公式表达: **优化后的执行时间 = 受优化影响的执行时间 / 加速倍率 + 不受影响的执行时间**</span>
